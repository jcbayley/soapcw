{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Viterbi Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The Viterbi algorithm is an efficient method for determining the most probable set of states (a single 'track' of steps on the time-frequency plane) in a Markov model dependent on data, where the model has a discrete number of states at each step. Rather than computing the probability of every possible track and selecting the most probable, the algorithm maximises this probability after every discrete step. As a result, a partial track which cannot ultimately be the most probable is rejected before the next step is calculated, and only a fraction of all possible tracks need to be computed to find the one that is most probable.\n",
    "\n",
    "\n",
    "In this work we apply the Viterbi algorithm to a GW strain time-series to find the most probable track of a single variable-frequency signal in the noisy data.  We divide the time series into $N$ equal-length and contiguous segments ${\\mathbf x}_j$,  defining the set $D \\equiv \\{{\\mathbf x}_j\\}$. The 'states' in the model correspond to the frequencies a signal could have in each segment. A 'track' is a list of such frequencies ${\\mathbf \\nu}\\equiv \\{\\nu_j\\}$, where  $\\nu_j$ is the frequency in the segment ${\\mathbf x_j}$.\n",
    "\n",
    "\n",
    "Our objective is to calculate the most probable track given the data, i.e., the\n",
    "track that maximises $p({\\mathbf \\nu}\\mid D)$. Using Bayes theorem, this posterior probability can\n",
    "be written as\n",
    "\n",
    "\\begin{equation}\n",
    "p({\\mathbf \\nu} \\mid D) = \\frac{p({\\mathbf \\nu})p(D \\mid {\\mathbf \\nu})}{p(D)},\n",
    "\\end{equation}\n",
    "\n",
    "where $p({\\mathbf \\nu}) $ is the prior probability of the\n",
    "track, $p(D \\mid{\\mathbf \\nu})$ is the likelihood of the track (i.e., the\n",
    "probability of the data given the track) and $p(D)$ is the model evidence (or\n",
    "marginalised likelihood).\n",
    "\n",
    "The Viterbi algorithm treats the track as the result of a Markovian process,\n",
    "such that the current state depends only on the previous state. It is\n",
    "therefore useful to split the track's prior into a set of transition\n",
    "probabilities such that\n",
    "\n",
    "\\begin{align}\n",
    "p({\\mathbf \\nu}) &= p(\\nu_{N - 1}, \\ldots, \\nu_1, \\nu_0)\\nonumber \\\\\n",
    "&=  p(\\omega_{N} \\mid \\omega_{N-1}, \\ldots, \\omega_1,\\omega_0)p(\\omega_{N-1} \\mid \\omega_{N-2}, \\ldots, \\omega_1,\\omega_0) \\ldots p(\\omega_1 \\mid \\omega_0)p(\\omega_0) \\\\\n",
    "&= p(\\nu_{N - 1} \\mid \\nu_{N-2})p(\\nu_{N-2} \\mid \\nu_{N-3}) \\dots p(\\nu_1 \\mid \\nu_0)p(\\nu_0) \\nonumber \\\\\n",
    "&= p(\\nu_0)\\prod_{j=1}^{N-1}p(\\nu_{j} \\mid \\nu_{j-1}),\n",
    "\\end{align}\n",
    "\n",
    "where $p(\\nu_0)$ is the prior probability that the signal in the first time\n",
    "step has a frequency $\\nu_0$ and $p(\\nu_{j} \\mid \\nu_{j-1})$ is the\n",
    "prior 'transition' probability for $\\nu_j$ given the frequency at the last\n",
    "step was $\\nu_{j-1}$.\n",
    "\n",
    "The noise in each of the segments can be treated as independent, therefore we can write,\n",
    "\n",
    "\\begin{equation}\n",
    "p(D \\mid {\\mathbf \\nu}) = \\prod_{j=0}^{N-1}p({\\mathbf x_j} \\mid \\nu_j),\n",
    "\\end{equation}\n",
    "\n",
    "where $p({\\mathbf x_j} \\mid \\nu_j)$ is the likelihood of our\n",
    "signal having a frequency $\\nu_j$ in the $j$th segment.\n",
    "\n",
    "Using the above equations, the posterior probability is then\n",
    "\n",
    "\\begin{equation}\n",
    "p({\\mathbf \\nu} | D) =\n",
    "\\frac{p(\\nu_0)p({ \\mathbf x_0} | \\nu_0) \\displaystyle\\prod_{j=1}^{N-1}p(\\nu_{j}\n",
    "| \\nu_{j-1})p({\\mathbf x_j} | \\nu_j)}{\\displaystyle\\sum_{S}\n",
    "\\left\\{p(\\nu_0)p({\\mathbf x_0} | \\nu_0)\\displaystyle\\prod_{j=1}^{N-1}p(\\nu_{j} |\n",
    "\\nu_{j-1})p({\\mathbf x_j} | \\nu_j)\\right\\}} ,\n",
    "\\end{equation}\n",
    "\n",
    "where in the denominator we must sum over all possible tracks\n",
    "$S$. We require the specific track, or set of frequencies, $\\hat{\\mathbf\n",
    "\\nu}$ that  maximises the posterior probability, and which therefore\n",
    "maximises the numerator  on the right-hand side of  the previous equation, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\hat{\\mathbf \\nu} | D) \\propto \\max_{\\mathbf \\nu}{\\left[p(\\nu_0)p({\\mathbf x_0} |\n",
    "\\nu_0) \\prod_{j=1}^{N-1}p(\\nu_{j} |\\nu_{j-1})p({\\mathbf x_j} | \\nu_j)\\right]}.\n",
    "\\end{equation}\n",
    "\n",
    "This track also maximises the log of the probability,\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\log p(\\hat{\\mathbf \\nu} | D)  = \\max_{{\\mathbf \\nu}}{\\biggl\\{ \\log p(\\nu_0) + \\log p({\\mathbf x_0} | \\nu_0)  } \\\\\n",
    "\\left. \\sum_{j=1}^{N-1} \\biggl[ \\log p(\\nu_{j} | \\nu_{j-1}) + \\log p({\\mathbf x_j}\n",
    "| \\nu_j) \\biggr] \\right\\} + \\text{const}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "The Viterbi algorithm finds the most probable track $\\hat{\\mathbf \\nu}$ by calculating the quantities in the previous equation for each frequency at each time step. In the following sections we explain how this is achieved in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soap",
   "language": "python",
   "name": "soap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
